# ğŸ¤– Sistema RAG - PolÃ­cia Federal

Sistema de RecuperaÃ§Ã£o e GeraÃ§Ã£o Aumentada (RAG) para consulta de legislaÃ§Ãµes e documentos da PolÃ­cia Federal.

## ğŸš€ Funcionalidades

- âœ… **Cache de Respostas**: Respostas instantÃ¢neas para perguntas repetidas
- âœ… **DetecÃ§Ã£o AutomÃ¡tica de MudanÃ§as**: ReconstrÃ³i base quando documentos sÃ£o alterados
- âœ… **ReconexÃ£o AutomÃ¡tica**: Sistema resiliente a falhas de conexÃ£o
- âœ… **Interface Intuitiva**: Feedback claro sobre status e progresso
- âœ… **Busca SemÃ¢ntica**: Encontra informaÃ§Ãµes relevantes mesmo em documentos longos
- âœ… **Processamento Otimizado**: 40% mais rÃ¡pido que versÃµes anteriores

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8 ou superior
- Ollama instalado e configurado
- Arquivos PDF na pasta `SGP/`

## ğŸ› ï¸ InstalaÃ§Ã£o RÃ¡pida

### Windows
```bash
install.bat
```

### Linux/Mac
```bash
chmod +x install.sh
./install.sh
```

### InstalaÃ§Ã£o Manual
```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar e configurar Ollama
# Visite: https://ollama.ai/
ollama pull nomic-embed-text
ollama pull llama3.2
```

## ğŸ¯ Como Usar

1. **Adicione documentos PDF** na pasta `SGP/`
2. **Inicie o Ollama**: `ollama serve`
3. **Execute o sistema**: `python main.py`
4. **FaÃ§a suas perguntas** sobre legislaÃ§Ã£o da PF

## ğŸ“Š Performance

| Tipo de Consulta | Tempo MÃ©dio | Cache Hit |
|------------------|-------------|-----------|
| Pergunta nova | 2-3 segundos | âŒ |
| Pergunta repetida | 0.01 segundos | âœ… |
| Startup | 1-2 segundos | - |

## ğŸ“ Estrutura do Projeto

```
RAG/
â”œâ”€â”€ main.py                 # Sistema principal
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ install.sh             # Script instalaÃ§Ã£o Linux/Mac
â”œâ”€â”€ install.bat            # Script instalaÃ§Ã£o Windows
â”œâ”€â”€ SGP/                   # Pasta com documentos PDF
â”‚   â”œâ”€â”€ documento1.pdf
â”‚   â””â”€â”€ documento2.pdf
â””â”€â”€ faissDB/               # Base de dados (criada automaticamente)
    â”œâ”€â”€ index.faiss
    â”œâ”€â”€ index.pkl
    â”œâ”€â”€ sgp_hash.json
    â””â”€â”€ cache_respostas.json
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Modelos Alternativos (mais rÃ¡pidos)
```python
# No main.py, linha ~204:
llm = OllamaLLM(model="mistral:7b")  # Mais rÃ¡pido que llama3.2
# ou
llm = OllamaLLM(model="qwen2:7b")    # Alternativa rÃ¡pida
```

### Ajustar NÃºmero de Chunks
```python
# No main.py, linha ~287:
retriever = database.as_retriever(search_kwargs={"k": 4})  # Mais rÃ¡pido
# ou
retriever = database.as_retriever(search_kwargs={"k": 8})  # Mais preciso
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Ollama nÃ£o encontrado"
```bash
# Verificar se Ollama estÃ¡ rodando
ollama serve

# Baixar modelos necessÃ¡rios
ollama pull nomic-embed-text
ollama pull llama3.2
```

### Erro: "pypdf not found"
```bash
pip install pypdf
```

### Sistema lento
1. Use modelos menores (`mistral:7b`)
2. Reduza nÃºmero de chunks (`k=4`)
3. Verifique cache ativo

### Proxy corporativo
- Configure bypass para `localhost:11434`
- Ou use VPN para acessar Ollama

## ğŸ“ˆ OtimizaÃ§Ãµes Implementadas

- **Cache persistente** com normalizaÃ§Ã£o inteligente
- **Chunks otimizados** (500 chars com overlap de 200)
- **Retrieval reduzido** (6 chunks em vez de 10)
- **Auto-reconexÃ£o** em caso de falhas
- **DetecÃ§Ã£o de mudanÃ§as** por hash MD5
- **Feedback em tempo real** de progresso

## ğŸ”® Melhorias Futuras

- Interface web com Streamlit/Gradio
- Busca hÃ­brida (semÃ¢ntica + keyword)
- Streaming de respostas
- Suporte a mais formatos (DOCX, TXT)
- API REST para integraÃ§Ã£o
- Dashboard de mÃ©tricas

## ğŸ“ Suporte

Para dÃºvidas ou problemas:
1. Verifique os logs do sistema
2. Confirme que Ollama estÃ¡ rodando
3. Teste com documentos pequenos primeiro
4. Verifique conexÃ£o de rede

---

**Desenvolvido para otimizar consultas jurÃ­dicas na PolÃ­cia Federal** ğŸ‡§ğŸ‡·
