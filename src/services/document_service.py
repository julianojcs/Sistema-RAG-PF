"""
ServiÃ§o para gerenciamento de documentos e base de dados
Refatorado para usar pipeline PF RAG (PDF -> normalizaÃ§Ã£o -> parsing -> chunking -> indexaÃ§Ã£o FAISS)
"""
import os
import sys
from typing import List, Optional
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings

from ..config.settings import Settings
from ..utils.file_utils import FileUtils
from .ollama_service import OllamaService

# PF pipeline
from src.pf_rag.io_pdf import extract_text as pf_extract_text
from src.pf_rag.normalize import clean_text as pf_clean_text
from src.pf_rag.parse_norma import detect_structure as pf_detect
from src.pf_rag.metadata_pf import extract as pf_meta_extract
from src.pf_rag.chunker import build_chunks as pf_build_chunks
from src.pf_rag.embed_index import Indexer as PFIndexer, SbertEmbeddings
from src.vector_backends.qdrant_backend import QdrantIndexer
from src.pf_rag.export_jsonl import export_chunks_jsonl


class DocumentService:
    """ServiÃ§o para processamento de documentos e criaÃ§Ã£o da base de dados"""

    def __init__(self):
        self.database: Optional[object] = None
        # Embeddings serÃ£o escolhidos conforme backend configurado
        if Settings.EMBEDDING_BACKEND == "sbert":
            self.embeddings = SbertEmbeddings()
        else:
            self.embeddings = OllamaEmbeddings(model=Settings.EMBEDDING_MODEL)

    def load_or_create_database(self, progress_callback=None) -> Optional[object]:
        """Carrega base existente ou cria nova se necessÃ¡rio"""
        print("âš¡ Verificando base de dados...")

        # Verifica se houve mudanÃ§as na pasta SGP
        mudancas_detectadas, hash_atual = FileUtils.check_folder_changes()

        if mudancas_detectadas:
            if os.path.exists(Settings.FAISS_DB_PATH):
                print("ğŸ”„ MudanÃ§as detectadas na pasta SGP. Recriando base de dados...")
            else:
                print("ğŸ“ Primeira execuÃ§Ã£o. Criando base de dados...")
            self.database = None
        else:
            self.database = self._load_existing_database()
            if self.database:
                print("âœ… Base de dados carregada (sem mudanÃ§as)!")
                return self.database

        # Se chegou aqui, precisa criar nova base (PF pipeline)
        self.database = self._create_pf_rag_database(progress_callback)
        if self.database:
            FileUtils.save_hash(hash_atual)

        return self.database

    def _load_existing_database(self) -> Optional[object]:
        """Carrega base de dados existente"""
        try:
            if Settings.VECTOR_DB_BACKEND == "qdrant":
                q = QdrantIndexer(backend=Settings.EMBEDDING_BACKEND)
                return q.load_qdrant()
            # Default: FAISS
            return FAISS.load_local(
                Settings.FAISS_DB_PATH,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
        except Exception:
            print("âš ï¸ Erro ao carregar base de dados. Recriando...")
            return None

    def _create_pf_rag_database(self, progress_callback=None) -> Optional[object]:
        """Cria nova base FAISS usando pipeline PF (hierÃ¡rquico)."""
        print("ğŸ“„ Processando documentos da pasta SGP com pipeline PF RAG...")
        try:
            arquivos_pdf = FileUtils.get_pdf_files()
            if not arquivos_pdf:
                print("âŒ Nenhum documento PDF encontrado na pasta SGP/")
                print("ğŸ“ Adicione arquivos PDF vÃ¡lidos na pasta SGP/ e reinicie o programa")
                sys.exit(1)

            import time
            all_chunks = []
            total_files = len(arquivos_pdf)
            
            for idx, pdf in enumerate(arquivos_pdf):
                if progress_callback:
                    file_progress = idx / total_files * 0.6  # 60% para processamento PDFs
                    progress_callback(file_progress, f"Processando {os.path.basename(pdf)} ({idx+1}/{total_files})")
                
                t0 = time.time()
                raw, pages, ocr = pf_extract_text(pdf)
                text, pages2 = pf_clean_text(raw, pages)
                nodes, heading = pf_detect(text)
                meta = pf_meta_extract(text, heading, os.path.basename(pdf))
                chunks = pf_build_chunks(nodes, text, meta, pdf, [p.index for p in pages2])
                all_chunks.extend(chunks)
                print(f"ğŸ“¦ {os.path.basename(pdf)} -> {len(chunks)} chunks em {time.time()-t0:.2f}s (OCR={ocr})")

            # Verifica conexÃ£o com Ollama somente se backend de embeddings for Ollama
            if Settings.EMBEDDING_BACKEND == "ollama":
                conectado, erro = OllamaService.check_connection()
                if not conectado:
                    print("âŒ Ollama nÃ£o estÃ¡ acessÃ­vel para criar embeddings")
                    OllamaService.print_connection_error(erro)
                    sys.exit(1)

            if Settings.VECTOR_DB_BACKEND.startswith("qdrant"):
                if progress_callback:
                    progress_callback(0.65, f"Preparando indexaÃ§Ã£o Qdrant ({len(all_chunks)} chunks)...")
                
                print(f"ğŸ§  Criando embeddings e base Qdrant (chunks={len(all_chunks)})...")
                qindex = QdrantIndexer(backend=Settings.EMBEDDING_BACKEND)
                
                # Clear any existing storage to avoid conflicts
                try:
                    qindex.clear_collection()
                except Exception:
                    pass
                    
                # Also try to remove the storage folder if it exists and is locked
                import shutil
                try:
                    if os.path.exists(Settings.QDRANT_PATH):
                        shutil.rmtree(Settings.QDRANT_PATH, ignore_errors=True)
                except Exception:
                    pass
                
                def qdrant_callback(frac: float, msg: str):
                    # Map to 65%-95% range
                    val = 0.65 + (frac * 0.3)
                    if progress_callback:
                        progress_callback(val, f"Qdrant: {msg}")
                    
                t0 = time.time()
                db = qindex.build_qdrant(all_chunks, progress_callback=qdrant_callback)
                print(f"â±ï¸ Tempo embeddings+index: {time.time()-t0:.2f}s")
                # Qdrant embutido persiste via path automaticamente
            else:
                if progress_callback:
                    progress_callback(0.65, f"Preparando indexaÃ§Ã£o FAISS ({len(all_chunks)} chunks)...")
                
                indexer = PFIndexer(backend=Settings.EMBEDDING_BACKEND)
                print(f"ğŸ§  Criando embeddings hierÃ¡rquicos e base FAISS (chunks={len(all_chunks)})...")
                
                def faiss_callback(frac: float, msg: str):
                    # Map to 65%-95% range
                    val = 0.65 + (frac * 0.3)
                    if progress_callback:
                        progress_callback(val, f"FAISS: {msg}")
                
                t0 = time.time()
                db = indexer.build_faiss(all_chunks, progress_callback=faiss_callback)
                print(f"â±ï¸ Tempo embeddings+index: {time.time()-t0:.2f}s")
                print("ğŸ’¾ Salvando base de dados...")
                indexer.save_faiss(db, Settings.FAISS_DB_PATH)
                
            # Export JSONL para auditoria
            if Settings.EXPORT_CHUNKS_JSONL:
                if progress_callback:
                    progress_callback(0.98, "Exportando JSONL para auditoria...")
                try:
                    msg = export_chunks_jsonl(all_chunks, Settings.CHUNKS_JSONL_PATH)
                    print("ğŸ“", msg)
                except Exception as e:
                    print(f"âš ï¸ Falha ao exportar JSONL: {e}")
                    
            if progress_callback:
                progress_callback(1.0, "âœ… Base de dados criada com sucesso!")
            print("âœ… Base de dados criada (PF RAG)!")
            return db
        except Exception as e:
            print(f"âŒ Erro ao criar base PF RAG: {str(e)[:150]}...")
            sys.exit(1)

    def _load_documents(self) -> List:
        """Carrega documentos PDF da pasta SGP"""
        try:
            # Verifica arquivos PDF
            arquivos_pdf = FileUtils.get_pdf_files()
            print(f"ğŸ“ Encontrados {len(arquivos_pdf)} arquivos PDF: {[os.path.basename(f) for f in arquivos_pdf]}")

            # Carrega documentos (cada pÃ¡gina vira um documento)
            loader = DirectoryLoader(f"{Settings.PDF_FOLDER}/", glob="*.pdf", loader_cls=PyPDFLoader)
            docs = loader.load()

            if not docs:
                print("âŒ Nenhum documento PDF encontrado na pasta SGP/")
                print("ğŸ“ Adicione arquivos PDF vÃ¡lidos na pasta SGP/ e reinicie o programa")
                sys.exit(1)

            print(f"ğŸ“‹ Total de pÃ¡ginas carregadas: {len(docs)}")
            return docs

        except ImportError as e:
            if "pypdf" in str(e):
                print("âŒ Biblioteca pypdf nÃ£o encontrada")
                print("ğŸ”§ Execute: pip install pypdf")
            else:
                print(f"âŒ Erro de importaÃ§Ã£o: {str(e)}")
            sys.exit(1)

        except Exception as e:
            print(f"âŒ Erro ao carregar documentos: {str(e)[:100]}...")
            print("ğŸ”§ Verifique se os arquivos PDF nÃ£o estÃ£o corrompidos")
            sys.exit(1)

    def _split_documents(self, docs: List) -> List:
        """Divide documentos em chunks"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Settings.CHUNK_SIZE,
            chunk_overlap=Settings.CHUNK_OVERLAP,
            separators=Settings.get_separators()
        )

        print("ğŸ”§ Dividindo documentos em chunks...")
        documents = text_splitter.split_documents(docs)
        print(f"ğŸ“Š Criados {len(documents)} chunks de texto")

        return documents

    def _create_faiss_database(self, documents: List) -> Optional[FAISS]:
        """Cria base FAISS com embeddings"""
        print("ğŸ§  Criando embeddings e base de dados...")
        print("âš ï¸ Este processo pode demorar alguns minutos...")

        try:
            # Verifica conexÃ£o com Ollama
            conectado, erro = OllamaService.check_connection()
            if not conectado:
                print("âŒ Ollama nÃ£o estÃ¡ acessÃ­vel para criar embeddings")
                OllamaService.print_connection_error(erro)
                sys.exit(1)

            print("âœ… Ollama acessÃ­vel, criando embeddings...")
            database = FAISS.from_documents(documents, self.embeddings)

            print("ğŸ’¾ Salvando base de dados...")
            database.save_local(Settings.FAISS_DB_PATH)
            print("âœ… Base de dados criada e hash salvo!")

            return database

        except Exception as e:
            error_str = str(e)
            print("âŒ Erro ao criar base de dados:")

            if "Connection refused" in error_str or "503" in error_str:
                print("ğŸ”§ Ollama perdeu conexÃ£o durante o processamento")
                print("ğŸ’¡ Execute 'ollama serve' e tente novamente")
            elif "proxy" in error_str.lower():
                print("ğŸŒ Bloqueio de proxy detectado")
                print("ğŸ’¡ Configure bypass para localhost:11434")
            else:
                print(f"ğŸ“ Detalhes: {error_str[:150]}...")

            sys.exit(1)
